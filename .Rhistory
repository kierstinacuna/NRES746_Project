guesses[counter,] <- oldguess
}
image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-135),add=T)
lines(guesses,col="red")
k <- 100
oldguess <- startingvals
counter <- 0
guesses <- matrix(0,nrow=10000,ncol=2)
colnames(guesses) <- names(startingvals)
MLE <- list(vals=startingvals,lik=-GammaLikelihoodFunction(startingvals),step=0)
while(counter<10000){
newguess <- newGuess(oldguess)
while(any(newguess<0)) newguess <- newGuess(oldguess)
loglikdif <- LikDif(oldguess,newguess)
if(loglikdif>0){
oldguess <- newguess
}else{
rand=runif(1)
if(rand <= exp(loglikdif/k)){
oldguess <- newguess   # accept even if worse!
}
}
counter <- counter + 1
if(counter%%100==0) k <- k*0.8
guesses[counter,] <- oldguess
thislik <- -GammaLikelihoodFunction(oldguess)
if(thislik>MLE$lik) MLE <- list(vals=oldguess,lik=-GammaLikelihoodFunction(oldguess),step=counter)
}
image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-135),add=T)
lines(guesses,col="red")
points(MLE$vals[1],MLE$vals[2],col="green",pch=20,cex=3)
MLE
optim(params,GammaLikelihoodFunction)$par
install.packages(c("httpuv", "utf8", "wk"))
library(emdbook)
data(FirDBHFec)
fir <- na.omit(FirDBHFec[,c("TOTCONES","DBH","WAVE_NON")])
fir$TOTCONES <- round(fir$TOTCONES)
head(fir)
plot(fir$TOTCONES ~ fir$DBH)   # fecundity as a function of tree size (diameter at breast height)
ndx <- fir$WAVE_NON=="w"   # logical vector indicating which observations were from "wave" sites
plot(fir$TOTCONES[ndx] ~ fir$DBH[ndx],xlab="DBH",ylab="Tot Cones")
points(fir$DBH[!ndx],fir$TOTCONES[!ndx],pch=4,col="red")
plot(fir$TOTCONES ~ fir$DBH)   # fecundity as a function of tree size (diameter at breast height)
plot(fir$TOTCONES ~ fir$DBH, xlim = c(0,18, ylim = c(0,300)))   # fecundity as a function of tree size (diameter at breast height)
plot(fir$TOTCONES ~ fir$DBH, xlim = c(0,18), ylim = c(0,300)))   # fecundity as a function of tree size (diameter at breast height)
plot(fir$TOTCONES ~ fir$DBH, xlim = c(0,18), ylim = c(0,300))   # fecundity as a function of tree size (diameter at breast height)
plot(fir$TOTCONES ~ fir$DBH, xlim = c(3,18), ylim = c(0,300))   # fecundity as a function of tree size (diameter at breast height)
ndx <- fir$WAVE_NON=="w"   # logical vector indicating which observations were from "wave" sites
ndx <- fir$WAVE_NON=="w"   # logical vector indicating which observations were from "wave" sites
plot(fir$TOTCONES[ndx] ~ fir$DBH[ndx],xlab="DBH",ylab="Tot Cones", xlim = c(3,18), ylim = c(0,300))
points(fir$DBH[!ndx],fir$TOTCONES[!ndx],pch=4,col="red")
NegBinomLik_full <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos    # note: we are hard-coding the data into our likelihood function here!
a <- c(params[1],params[2])[wave.code]     # a parameters (two for wave and one for non-wave)
b <- c(params[3],params[4])[wave.code]      # b parameter (two for wave and one for non-wave)
k <- c(params[5],params[6])[wave.code]       # over-dispersion parameters (two for wave and one for non-wave)
expcones <- a*fir$DBH^b   # expected number of cones (deterministic component)
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))     # add stochastic component: full data likelihood
}
params <- c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1)
NegBinomLik_full(params)
MLE_full <- optim(fn=NegBinomLik_full,par=c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1),method="L-BFGS-B")
MLE_full$par
MLE_full$value
NegBinomLik_constb <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
a <- c(params[1],params[2])[wave.code]      # a parameters
b <- params[3]                              # b parameter (not a function of wave/nonwave)
k <- c(params[4],params[5])[wave.code]      # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a.n=1,a.w=1,b=1,k.n=1,k.w=1)
NegBinomLik_constb(params)
MLE_constb <- optim(fn=NegBinomLik_constb,par=c(a.n=1,a.w=1,b=1,k.n=1,k.w=1),method="L-BFGS-B")
MLE_constb$par
MLE_constb$value
ms_full <- 2*MLE_full$value     # this is 2 * min.nll = -2*logLik_at_MLE
ms_constb <- 2*MLE_constb$value
ms_full
ms_constb
Deviance <- ms_constb - ms_full
Deviance
Chisq.crit <- qchisq(0.95,1)
Chisq.crit
NegBinomLik_constak <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
a <- params[1]                             # a parameters
b <- c(params[2],params[3])[wave.code]                              # b parameter (not a function of wave/nonwave)
k <- params[4]                               # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a=1,b.n=1,b.w=1,k=1)
NegBinomLik_constak(params)
MLE_constak <- optim(fn=NegBinomLik_constak,par=params)
MLE_constak$par
MLE_constak$value
ms_constak <- 2*MLE_constak$value
PoisLik_nowave <- function(params){
a <- params[1]      # a parameters
b <- params[2]      # b parameter (not a function of wave/nonwave)
expcones <- a*fir$DBH^b
-sum(dpois(fir$TOTCONES,lambda=expcones,log=TRUE))
}
params <- c(a=1,b=1)
PoisLik_nowave(params)
MLE_pois <- optim(fn=PoisLik_nowave,par=params)
MLE_pois$par
MLE_pois$value
ms_pois <- 2*MLE_pois$value
AIC_constak <- ms_constak + 2*4
AIC_full <- ms_full + 2*6
AIC_constb <- ms_constb + 2*5
AIC_nowave <- ms_nowave + 2*3
AIC_pois <- ms_pois + 2*2
AIC_nowave <- ms_nowave + 2*3
ms_full <- 2*MLE_full$value
ms_nowave <- 2*MLE_nowave$value
library(emdbook)
data(FirDBHFec)
fir <- na.omit(FirDBHFec[,c("TOTCONES","DBH","WAVE_NON")])
fir$TOTCONES <- round(fir$TOTCONES)
head(fir)
plot(fir$TOTCONES ~ fir$DBH, xlim = c(3,18), ylim = c(0,300))   # fecundity as a function of tree size (diameter at breast height)
ndx <- fir$WAVE_NON=="w"   # logical vector indicating which observations were from "wave" sites
plot(fir$TOTCONES[ndx] ~ fir$DBH[ndx],xlab="DBH",ylab="Tot Cones", xlim = c(3,18), ylim = c(0,300))
points(fir$DBH[!ndx],fir$TOTCONES[!ndx],pch=4,col="red")
legend("topleft",pch=c(1,4),col=c("black","red"),legend=c("Wave","Non-wave"),bty="n")
NegBinomLik_full <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos    # note: we are hard-coding the data into our likelihood function here!
a <- c(params[1],params[2])[wave.code]     # a parameters (two for wave and one for non-wave)
b <- c(params[3],params[4])[wave.code]      # b parameter (two for wave and one for non-wave)
k <- c(params[5],params[6])[wave.code]       # over-dispersion parameters (two for wave and one for non-wave)
expcones <- a*fir$DBH^b   # expected number of cones (deterministic component)
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))     # add stochastic component: full data likelihood
}
params <- c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1)
NegBinomLik_full(params)
MLE_full <- optim(fn=NegBinomLik_full,par=c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1),method="L-BFGS-B")
MLE_full$par
MLE_full$value
NegBinomLik_constb <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
a <- c(params[1],params[2])[wave.code]      # a parameters
b <- params[3]                              # b parameter (not a function of wave/nonwave)
k <- c(params[4],params[5])[wave.code]      # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a.n=1,a.w=1,b=1,k.n=1,k.w=1)
NegBinomLik_constb(params)
MLE_constb <- optim(fn=NegBinomLik_constb,par=c(a.n=1,a.w=1,b=1,k.n=1,k.w=1),method="L-BFGS-B")
MLE_constb$par
MLE_constb$value
ms_full <- 2*MLE_full$value     # this is 2 * min.nll = -2*logLik_at_MLE
ms_constb <- 2*MLE_constb$value
ms_full
ms_constb
Deviance <- ms_constb - ms_full
Deviance
Chisq.crit <- qchisq(0.95,1)
Chisq.crit
Deviance>=Chisq.crit   # perform the LRT
1-pchisq(Deviance,1)   # p-value
curve(dchisq(x,df=1),0,5)
abline(v=Deviance,col="red",lwd=4)
NegBinomLik_nowave <- function(params){
a <- params[1]      # a parameters
b <- params[2]      # b parameter (not a function of wave/nonwave)
k <- params[3]      # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a=1,b=1,k=1)
NegBinomLik_nowave(params)
MLE_nowave <- optim(fn=NegBinomLik_nowave,par=params,method="L-BFGS-B")
MLE_nowave$par
MLE_nowave$value
ms_full <- 2*MLE_full$value
ms_nowave <- 2*MLE_nowave$value
Deviance <- ms_nowave - ms_full
Deviance
Chisq.crit <- qchisq(0.95,df=3)   # now three additional params in the more complex model!
Chisq.crit
Deviance>=Chisq.crit
1-pchisq(Deviance,df=3)   # p-value
# Visualize the likelihood ratio test (test statistic and sampling distribution under the null)
curve(dchisq(x,df=3),0,15)
abline(v=Deviance,col="red",lwd=4)
NegBinomLik_constak <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
a <- params[1]                             # a parameters
b <- c(params[2],params[3])[wave.code]                              # b parameter (not a function of wave/nonwave)
k <- params[4]                               # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a=1,b.n=1,b.w=1,k=1)
NegBinomLik_constak(params)
MLE_constak <- optim(fn=NegBinomLik_constak,par=params)
MLE_constak$par
MLE_constak$value
ms_constak <- 2*MLE_constak$value
PoisLik_nowave <- function(params){
a <- params[1]      # a parameters
b <- params[2]      # b parameter (not a function of wave/nonwave)
expcones <- a*fir$DBH^b
-sum(dpois(fir$TOTCONES,lambda=expcones,log=TRUE))
}
params <- c(a=1,b=1)
PoisLik_nowave(params)
MLE_pois <- optim(fn=PoisLik_nowave,par=params)
MLE_pois$par
MLE_pois$value
ms_pois <- 2*MLE_pois$value
AIC_constak <- ms_constak + 2*4
AIC_full <- ms_full + 2*6
AIC_constb <- ms_constb + 2*5
AIC_nowave <- ms_nowave + 2*3
AIC_pois <- ms_pois + 2*2
AICtable <- data.frame(
Model = c("Full","Constant b","Constant a and k","All constant","Poisson"),
AIC = c(AIC_full,AIC_constb,AIC_constak,AIC_nowave,AIC_pois),
LogLik = c(ms_full/-2,ms_constb/-2,ms_constak/-2,ms_nowave/-2,ms_pois/-2),
params = c(6,5,4,3,2),
stringsAsFactors = F
)
AICtable$DeltaAIC <- AICtable$AIC-AICtable$AIC[which.min(AICtable$AIC)]
AICtable$Weights <- round(exp(-0.5*AICtable$DeltaAIC) / sum(exp(-0.5*AICtable$DeltaAIC)),3)
AICtable$AICc <- AICtable$AIC + ((2*AICtable$params)*(AICtable$params+1))/(nrow(fir)-AICtable$params-1)
AICtable[order(AICtable$AIC),c(1,7,2,5,6,4,3)]
probs1 <- dbinom(0:10,10,0.5)
names(probs1) = 0:10
barplot(probs1,ylab="probability")
dbinom(2,10,0.5)
## Now we can consider a model whereby "p" is a free parameter
curve(dbeta(x,1,1))  # uniform prior on "p"
# ?integrate
binom2 <- function(x) dbinom(x=2,size=10,prob=x)
marginal_likelihood <- integrate(f=binom2,0,1)$value    # use "integrate" function in R
marginal_likelihood  # equal to 0.0909 = 1/11
binom3 <- function(x) dbinom(x=3,size=10,prob=x)
marginal_likelihood <- integrate(f=binom3,0,1)$value    # use "integrate" function
marginal_likelihood   # equal to 0.0909 = 1/11
AICtable <- data.frame(
Model = c("Full","Constant b","Constant a and k","All constant","Poisson"),
AIC = c(AIC_full,AIC_constb,AIC_constak,AIC_nowave,AIC_pois),
LogLik = c(ms_full/-2,ms_constb/-2,ms_constak/-2,ms_nowave/-2,ms_pois/-2),
params = c(6,5,4,3,2),
stringsAsFactors = F
)
AICtable$DeltaAIC <- AICtable$AIC-AICtable$AIC[which.min(AICtable$AIC)]
AICtable$Weights <- round(exp(-0.5*AICtable$DeltaAIC) / sum(exp(-0.5*AICtable$DeltaAIC)),3)
AICtable$AICc <- AICtable$AIC + ((2*AICtable$params)*(AICtable$params+1))/(nrow(fir)-AICtable$params-1)
AICtable[order(AICtable$AIC),c(1,7,2,5,6,4,3)]
data(dune)
library(tidyverse)
library(vegan)
# Download Data
data(dune)
View(dune)
# Download Data
?dune
data(dune.env)
View(dune.env)
View(dune)
View(dune.env)
hist(dune$Achimill)
max(dune)
dune[whichmax(dune)]
dune[which(max(dune))]
which(max(dune)
hist(dune$Agrostol)
hist(dune$Agrostol)
hist(dune$Agrostol, freq= T)
hist(dune$Agrostol, freq= F)
library(emdbook)
data(FirDBHFec)
fir <- na.omit(FirDBHFec[,c("TOTCONES","DBH","WAVE_NON")])
fir$TOTCONES <- round(fir$TOTCONES)
head(fir)
library(glmmTMB)
library(DHARMa)
slugs<-read.table( 'http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/slugsurvey.txt', header=TRUE)
head(slugs)
out <- table(slugs$slugs,slugs$field)
out
library(glmmTMB)
library(DHARMa)
slugs<-read.table( 'http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/slugsurvey.txt', header=TRUE)
head(slugs)
out <- table(slugs$slugs,slugs$field)
out
barplot(t(out), beside=TRUE, angle=c(45,135), density=c(20,20), col=c('black','red'), legend.text=TRUE, xlab='# of slugs', ylab='frequency')
coords<-barplot(t(out), beside=TRUE, angle=c(45,135), density=c(20,20), ylim=c(0,27), col=c('black','red'), xlab='# of slugs', ylab='frequency')
box()
legend(coords[1,8], 26, c('nursery','rookery'), density=c(20,20), angle=c(45,135), fill=c('black','red'), cex=c(.8,.8),bty='n')
mod8 <- glmTMB(slugs ~ field, slugs, family = poisson(), ziformula = ~1)
library(glmmTMB)
library(DHARMa)
mod8 <- glmTMB(slugs ~ field, slugs, family = poisson(), ziformula = ~1)
mod8 <- glmmTMB(slugs ~ field, slugs, family = poisson(), ziformula = ~1)
summary(mod8)
logLik(mod8)
mod9 <- glmmTMB(slugs ~ 1, slugs, family = poisson(), ziformula = ~field)
logLik(mod9)
AIC(mod9)
library(DHARMa)
DHARMa::simulateResiduals(mod8)
res <- DHARMa::simulateResiduals(mod9)
View(res)
View(res)
DHARMa::testResiduals(res)
res <- DHARMa::simulateResiduals(mod9)
DHARMa::testResiduals(res)
mod10 <- glmmTMB(slugs ~ 1, slugs, family = nbinom2(like = "log"), ziformula = ~field)
mod10 <- glmmTMB(slugs ~ 1, slugs, family = nbinom2(link = "log"), ziformula = ~field)
AIC(mod10)
res.mod10 <- simulateResiduals(mod10)
testResiduals(res.mod10)
mod11 <- glmmTMB(slugs ~ 1, slugs, family = nbinom2(link = "log"), ziformula = ~1)
res.mod11 <- simulateResiduals(mod11)
testResiduals(res.mod11)
AIC(mod11)
library(vegan)
data(dune)
data(dune.env)
hist(dune$Achimill)
hist(dune$Agrostol)
hist(dune$Airaprae)
hist(dune$Alopgeni)
ordiplot(rda(dune ~ ., data=dune.env), type = "text")
hist(dune$Achimill)
hist(dune$Agrostol)
hist(dune$Airaprae)
hist(dune$Alopgeni)
ordiplot(rda(dune ~ ., data=dune.env), type = "text")
x <- dune.env
y <- dune
i <- 1
i <- 1
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
View(preds)
vars <- dune.env
sp_abun <- dune
i <- 1
library(vegan)
data(dune)
data(dune.env)
vars <- dune.env
sp_abun <- dune
i <- 1
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
x <- dune.env
y <- dune
i <- 1
data(dune)
data(dune.env)
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
x <- dune.env
y <- dune
i <- 1
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
View(dune.env)
mod <- lm(y[,i] ~ x[,1] + x[,2] + x[,3] + x[,4] + x[,5])
View(mod)
mod$fitted.values
View(preds)
preds[,i] <- mod$fitted.values
View(dune)
plot(preds[,1])
plot(preds[,1])
points(y[,1], col = "red")
# Step 2: PCA on the fitted values
pca <- prcomp(preds)
rda_func <- function(x, y){
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
for (i in 1:ncol(y)) {
mod <- lm(y[,i] ~ x[,1] + x[,2] + x[,3] + x[,4] + x[,5])
preds[,i] <- mod$fitted.values
}
# Step 2: PCA on the fitted values
pca <- prcomp(preds)
biplot(pca)
}
test <- rda_func(x = dune, y = dune.env)
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
for (i in 1:ncol(y)) {
mod <- lm(y[,i] ~ x[,1] + x[,2] + x[,3] + x[,4] + x[,5])
preds[,i] <- mod$fitted.values
}
# Step 2: PCA on the fitted values
pca <- prcomp(preds)
View(pca)
biplot(pca)
View(dune.env)
test <- rda_func(x = dune, y = dune.env)
test <- rda_func(x = dune.env, y = dune)
rda_func(x = dune.env, y = dune)
points(y[,1], col = "red")
anova(mod)
rda
pca$x
pca$rotation
summary(pca)
summary(mod)
pca$x
pca$rotation
summary(pca)
x <- decostand(x, method = "hellinger")
x <- dune.env
y <- decostand(y, method = "hellinger")
View(y)
library(tidyverse)
library(vegan)
# Upload Data
species <- read.csv("workshop10-main/book-en/data/doubsspe.csv", row.names = 1)
species <- species[-8,]
vars <- read.csv("workshop10-main/book-en/data/doubsenv.csv", row.names = 1)
vars <- vars[-8,]
View(x)
rda(dune ~ dune.env)
rda(dune ~ ., dune.env)
summary(pca)
pca$center
pca$scale
pca$x
data("varespec")
View(varespec)
species <- read.csv("workshop10-main/book-en/data/doubsspe.csv", row.names = 1)
list.files()
setwd("C:/Users/mgeno/Documents/GitHub/NRES746_Project")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
list.files()
species <- read.csv("workshop10-main/book-en/data/doubsspe.csv", row.names = 1)
species <- species[-8,]
vars <- read.csv("workshop10-main/book-en/data/doubsenv.csv", row.names = 1)
vars <- vars[-8,]
vars
hist(species$CHA)
hist(species
hist(species)
hist(species$TRU)
hist(species$VAI)
ordiplot(rda(dune ~ ., data=dune.env), type = "text")
ordiplot(rda(dune ~ ., data=dune.env)
#, type = "text"
)
## Doubs data ----
hist(species$CHA)
# Doubs
ordiplot(rda(species ~ ., data = vars)
)
# Doubs
ordiplot(rda(species ~ ., data = vars),
type = "text"
)
vars <- decostand(vars, method = "hellinger")
ordiplot(rda(species ~ ., data = vars),
type = "text"
)
doubs_rda <- rda(species ~ ., data = vars)
View(doubs_rda)
## Dune Data ----
library(vegan)
data(dune)
data(dune.env)
## Doubs Data ----
species <- read.csv("workshop10-main/book-en/data/doubsspe.csv", row.names = 1)
species <- species[-8,]
vars <- read.csv("workshop10-main/book-en/data/doubsenv.csv", row.names = 1)
vars <- vars[-8,]
species <- decostand(species, method = "hellinger")
vars <- decostand(vars, method = "standardize")
View(vars)
View(varespec)
View(species)
doubs_rda <- rda(species ~ ., data = vars)
View(doubs_rda)
ordiplot(doubs_rda,
type = "text"
)
summary(doubs_rda)
pca(doubs_rda)
PCA(doubs_rda)
## Dune Data ----
library(vegan)
PCA(doubs_rda)
prcomp(doubs_rda)
View(vars)
rda_func <- function(x, y){
# Step 1: Regress species in y over vars in x
preds <- matrix(NA, nrow = nrow(y), ncol = ncol(y))
for (i in 1:ncol(y)) {
mod <- lm(y[,i] ~ x[,1] + x[,2] + x[,3] + x[,4] + x[,5] + x[,6] + x[,7] + x[,8] + x[,9] +
x[,10] + x[,11])
preds[,i] <- mod$fitted.values
}
# Step 2: PCA on the fitted values
pca <- prcomp(preds)
biplot(pca)
}
